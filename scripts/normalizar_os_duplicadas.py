#!/usr/bin/env python3
"""
Normalizador de OS Duplicadas
=============================

Consolida registros duplicados por OS NÂ°, unindo todos os dados disponÃ­veis
para criar registros Ãºnicos com mÃ¡ximo de informaÃ§Ãµes possÃ­vel.

EstratÃ©gia:
- Agrupa por OS NÂ°
- Para cada campo, escolhe o valor mais completo/recente
- Prioriza dados nÃ£o vazios
- Combina informaÃ§Ãµes fragmentadas
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import re

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class NormalizadorOS:
    def __init__(self):
        self.input_dir = Path("data/originais/oss/consolidadas")
        self.output_dir = Path("data/originais/oss/normalizadas")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.resultados = []
        self.estatisticas = {}
    
    def normalizar_arquivo(self, arquivo_consolidado):
        """Normaliza um arquivo consolidado, removendo duplicatas de OS"""
        loja_nome = arquivo_consolidado.stem.replace('_consolidado', '')
        logger.info(f"ğŸ”„ Normalizando loja: {loja_nome}")
        
        try:
            # Carregar dados
            df = pd.read_csv(arquivo_consolidado, encoding='utf-8-sig')
            
            registros_iniciais = len(df)
            logger.info(f"   ğŸ“Š Registros iniciais: {registros_iniciais}")
            
            # Verificar se tem coluna OS NÂ°
            if 'OS NÂ°' not in df.columns:
                logger.error(f"   âŒ Coluna 'OS NÂ°' nÃ£o encontrada em {loja_nome}")
                return None
            
            # Remover registros sem OS NÂ°
            df_limpo = df.dropna(subset=['OS NÂ°'])
            df_limpo = df_limpo[df_limpo['OS NÂ°'] != '']
            
            logger.info(f"   ğŸ“Š Registros com OS vÃ¡lida: {len(df_limpo)}")
            
            # Agrupar por OS NÂ° e consolidar
            logger.info(f"   ğŸ”„ Consolidando registros duplicados...")
            
            df_normalizado = self.consolidar_duplicatas(df_limpo, loja_nome)
            
            registros_finais = len(df_normalizado)
            duplicatas_removidas = registros_iniciais - registros_finais
            
            # EstatÃ­sticas
            stats = {
                'loja': loja_nome,
                'registros_iniciais': registros_iniciais,
                'registros_finais': registros_finais,
                'duplicatas_removidas': duplicatas_removidas,
                'taxa_consolidacao': round((duplicatas_removidas / registros_iniciais) * 100, 2) if registros_iniciais > 0 else 0,
                'os_unicas': len(df_normalizado['OS NÂ°'].unique()),
                'arquivo_saida': f"{loja_nome}_normalizado.csv"
            }
            
            self.estatisticas[loja_nome] = stats
            
            # Salvar arquivo normalizado
            arquivo_saida = self.output_dir / stats['arquivo_saida']
            df_normalizado.to_csv(arquivo_saida, index=False, encoding='utf-8-sig')
            
            logger.info(f"   âœ… Normalizado: {registros_finais} registros Ãºnicos")
            logger.info(f"   ğŸ“‰ Duplicatas removidas: {duplicatas_removidas} ({stats['taxa_consolidacao']}%)")
            logger.info(f"   ğŸ’¾ Salvo em: {stats['arquivo_saida']}")
            
            return stats
            
        except Exception as e:
            logger.error(f"   âŒ Erro ao normalizar {loja_nome}: {e}")
            return None
    
    def consolidar_duplicatas(self, df, loja_nome):
        """Consolida registros duplicados por OS NÂ°"""
        
        def consolidar_grupo(grupo):
            """Consolida um grupo de registros da mesma OS"""
            if len(grupo) == 1:
                return grupo.iloc[0]
            
            # Criar registro consolidado
            registro_consolidado = {}
            
            for coluna in grupo.columns:
                valores = grupo[coluna].dropna()
                valores = valores[valores != '']
                valores = valores[valores != 0]
                
                if len(valores) == 0:
                    registro_consolidado[coluna] = None
                elif len(valores) == 1:
                    registro_consolidado[coluna] = valores.iloc[0]
                else:
                    # MÃºltiplos valores - escolher o melhor
                    registro_consolidado[coluna] = self.escolher_melhor_valor(valores, coluna)
            
            return pd.Series(registro_consolidado)
        
        # Agrupar por OS NÂ° e consolidar
        df_normalizado = df.groupby('OS NÂ°').apply(consolidar_grupo).reset_index(drop=True)
        
        return df_normalizado
    
    def escolher_melhor_valor(self, valores, coluna):
        """Escolhe o melhor valor entre mÃºltiplos valores para uma coluna"""
        
        # Remover valores que sÃ£o claramente invÃ¡lidos
        valores_limpos = []
        for val in valores:
            val_str = str(val).strip()
            if val_str and val_str.lower() not in ['nan', 'none', 'null', '0', '0.0']:
                valores_limpos.append(val)
        
        if not valores_limpos:
            return None
        
        if len(valores_limpos) == 1:
            return valores_limpos[0]
        
        # EstratÃ©gias especÃ­ficas por tipo de coluna
        coluna_lower = coluna.lower()
        
        # Para campos de texto (nome, endereÃ§o, etc)
        if any(campo in coluna_lower for campo in ['nome', 'end', 'bairro', 'email']):
            # Escolher o mais longo (mais completo)
            return max(valores_limpos, key=lambda x: len(str(x)))
        
        # Para campos numÃ©ricos (CPF, RG, CEP, telefone)
        elif any(campo in coluna_lower for campo in ['cpf', 'rg', 'cep', 'telefone', 'celular']):
            # Escolher o mais completo (mais dÃ­gitos)
            valores_numericos = []
            for val in valores_limpos:
                digits = re.sub(r'\D', '', str(val))
                if digits:
                    valores_numericos.append((val, len(digits)))
            
            if valores_numericos:
                return max(valores_numericos, key=lambda x: x[1])[0]
        
        # Para datas
        elif any(campo in coluna_lower for campo in ['data', 'nasc', 'entr']):
            # Escolher a data mais recente vÃ¡lida
            datas_validas = []
            for val in valores_limpos:
                try:
                    if isinstance(val, (int, float)) and val > 40000:  # Excel date
                        datas_validas.append((val, val))
                    elif pd.to_datetime(val, errors='coerce') is not pd.NaT:
                        datas_validas.append((val, pd.to_datetime(val)))
                except:
                    continue
            
            if datas_validas:
                return max(datas_validas, key=lambda x: x[1])[0]
        
        # Para valores monetÃ¡rios
        elif any(campo in coluna_lower for campo in ['valor', 'total', 'sinal', 'resta']):
            # Escolher o maior valor vÃ¡lido
            valores_numericos = []
            for val in valores_limpos:
                try:
                    num_val = float(str(val).replace(',', '.'))
                    if num_val > 0:
                        valores_numericos.append((val, num_val))
                except:
                    continue
            
            if valores_numericos:
                return max(valores_numericos, key=lambda x: x[1])[0]
        
        # Default: escolher o primeiro valor nÃ£o vazio
        return valores_limpos[0]
    
    def processar_todas_lojas(self):
        """Processa todas as lojas consolidadas"""
        logger.info("ğŸš€ INICIANDO NORMALIZAÃ‡ÃƒO DE OS DUPLICADAS")
        logger.info(f"Data: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        logger.info("=" * 70)
        
        if not self.input_dir.exists():
            logger.error(f"âŒ DiretÃ³rio nÃ£o encontrado: {self.input_dir}")
            return
        
        # Buscar arquivos consolidados
        arquivos_consolidados = list(self.input_dir.glob("*_consolidado.csv"))
        
        if not arquivos_consolidados:
            logger.error("âŒ Nenhum arquivo consolidado encontrado")
            return
        
        logger.info(f"ğŸ“‚ Encontrados {len(arquivos_consolidados)} arquivo(s) consolidado(s)")
        logger.info("")
        
        # Processar cada arquivo
        for arquivo in sorted(arquivos_consolidados):
            resultado = self.normalizar_arquivo(arquivo)
            if resultado:
                self.resultados.append(resultado)
            logger.info("")
        
        self.gerar_relatorio()
    
    def gerar_relatorio(self):
        """Gera relatÃ³rio da normalizaÃ§Ã£o"""
        logger.info("=" * 70)
        logger.info("ğŸ“Š RELATÃ“RIO DE NORMALIZAÃ‡ÃƒO DE OS")
        logger.info("=" * 70)
        
        if not self.resultados:
            logger.error("âŒ Nenhuma loja foi normalizada com sucesso")
            return
        
        logger.info(f"ğŸª Lojas normalizadas: {len(self.resultados)}")
        
        total_iniciais = sum(r['registros_iniciais'] for r in self.resultados)
        total_finais = sum(r['registros_finais'] for r in self.resultados)
        total_duplicatas = sum(r['duplicatas_removidas'] for r in self.resultados)
        taxa_geral = round((total_duplicatas / total_iniciais) * 100, 2) if total_iniciais > 0 else 0
        
        logger.info(f"ğŸ“Š Registros iniciais: {total_iniciais:,}")
        logger.info(f"ğŸ“Š Registros finais: {total_finais:,}")
        logger.info(f"ğŸ“‰ Duplicatas consolidadas: {total_duplicatas:,} ({taxa_geral}%)")
        
        logger.info(f"\nâœ… RESULTADOS POR LOJA:")
        logger.info("-" * 50)
        
        for resultado in sorted(self.resultados, key=lambda x: x['duplicatas_removidas'], reverse=True):
            logger.info(f"\nğŸª {resultado['loja']}:")
            logger.info(f"   ğŸ“Š Inicial: {resultado['registros_iniciais']:,} â†’ Final: {resultado['registros_finais']:,}")
            logger.info(f"   ğŸ“‰ Duplicatas: {resultado['duplicatas_removidas']:,} ({resultado['taxa_consolidacao']}%)")
            logger.info(f"   ğŸ¯ OS Ãºnicas: {resultado['os_unicas']:,}")
            logger.info(f"   ğŸ“ Arquivo: {resultado['arquivo_saida']}")
        
        logger.info(f"\nğŸ“ ARQUIVOS NORMALIZADOS SALVOS EM:")
        logger.info(f"   ğŸ“ {self.output_dir}")
        
        logger.info(f"\nğŸ¯ MELHORES CONSOLIDAÃ‡Ã•ES:")
        top_consolidacoes = sorted(self.resultados, key=lambda x: x['taxa_consolidacao'], reverse=True)[:3]
        for i, resultado in enumerate(top_consolidacoes, 1):
            logger.info(f"   {i}. {resultado['loja']}: {resultado['taxa_consolidacao']}% consolidaÃ§Ã£o")
        
        logger.info(f"\nğŸš€ PRÃ“XIMOS PASSOS:")
        logger.info("-" * 30)
        logger.info("   1. Revisar dados normalizados")
        logger.info("   2. Validar qualidade da consolidaÃ§Ã£o")
        logger.info("   3. Importar para Supabase")
        logger.info("   4. Criar dashboard com dados limpos")
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ NormalizaÃ§Ã£o concluÃ­da! OS duplicadas consolidadas com sucesso!")

def main():
    normalizador = NormalizadorOS()
    normalizador.processar_todas_lojas()

if __name__ == "__main__":
    main()